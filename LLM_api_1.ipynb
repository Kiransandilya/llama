{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9043e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamaapi as L\n",
    "from llamaapi import comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edfab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7530715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d90ebff07343228df6f70d0ccc23fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ed30f54a2d4e5ab303c1541ced54f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335c68c921964ec8a70be7360a0ae123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d6c716da8d4918b340fffabacf66bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = L.LLM(\"vicuna\", \"13b\")\n",
    "b = L.LLM(\"llama2\", \"7b\")\n",
    "c = L.LLM(\"llama2\", \"13b\")\n",
    "d = L.LLM(\"falcon\", \"7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6e4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "56 \\* 24 = <<56*24=1344>>1344 sheets of paper. Answer: \\boxed{1344}.\n",
      "Solution: We have 24 folders, each containing 56 sheets of paper. To find the total number of sheets of paper, we need to multiply the number of folders by the number of sheets in each folder. This gives us 24 \\* 56 = 1344\n",
      "Response\n",
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "\n",
      "Answer: To find the total number of sheets of paper, you need to multiply the number of folders by the number of sheets in each folder.\n",
      "\n",
      "24 folders x 56 sheets/folder = 1,320 sheets of paper\n",
      "\n",
      "So, there are 1,320 sheets of paper altogether. üìùüì¶üî¢‚úçÔ∏èüëç #math #folders #sheets #papercraft\n",
      "Response\n"
     ]
    }
   ],
   "source": [
    "comparator(question,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433b185b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63580594fe0844829724a90ad721c3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "56 \\* 24 = <<56*24=1344>>1344 sheets of paper. Answer: \\boxed{1344}.\n",
      "Solution: We have 24 folders, each containing 56 sheets of paper. To find the total number of sheets of paper, we need to multiply the number of folders by the number of sheets in each folder. This gives us 24 \\* 56 = 1344\n",
      "Response\n"
     ]
    }
   ],
   "source": [
    "a = L.LLM(\"vicuna\", \"13b\")\n",
    "a.ask(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83231729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab7eff2a8334bb09aa4151739ecae53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "\n",
      "Answer: To find the total number of sheets of paper, you need to multiply the number of folders by the number of sheets in each folder.\n",
      "\n",
      "24 folders x 56 sheets/folder = 1,320 sheets of paper\n",
      "\n",
      "So, there are 1,320 sheets of paper altogether. üìùüì¶üî¢‚úçÔ∏èüëç #math #folders #sheets #papercraft\n",
      "Response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b = L.LLM(\"llama2\", \"7b\")\n",
    "\n",
    "\n",
    "b.ask(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ef9ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770a8e06e7624eff94ba84f0112680a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether? I'm confused on how to set this up.\n",
      "asked by Kaitlyn\n",
      "There are 1440 sheets of paper.\n",
      "posted by MsSue\n",
      "1440 sheets of paper.\n",
      "posted by Jasmine\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether? I'm confused on how to set this up. Please help!\n",
      "asked by kaitlyn on September 30, 2010\n",
      "Response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "c = L.LLM(\"llama2\", \"13b\")\n",
    "\n",
    "c.ask(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908b8658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a386e686c2b640688644e31af7645063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran.sandilya001/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "24 folders each has 56 sheets of paper inside them. How many sheets of paper are there altogether?\n",
      "- (Source:\n",
      "- (1.\n",
      "- (Reuters\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source:\n",
      "- (Source\n",
      "Response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = L.LLM(\"falcon\", \"7b\")\n",
    "\n",
    "\n",
    "d.ask(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702888e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
